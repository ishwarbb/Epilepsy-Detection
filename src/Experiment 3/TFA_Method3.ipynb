{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fTwy1ViNm71A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn import Module\n",
        "from torch.nn import Conv2d\n",
        "from torch.nn import Linear\n",
        "from torch.nn import MaxPool2d\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import LogSoftmax\n",
        "from torch.nn import Flatten\n",
        "from torch.nn import Flatten\n",
        "from torch.nn import Dropout\n",
        "from torch.nn import Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig0sMeTMpbW1",
        "outputId": "19e6a0c7-6c9e-4afe-fd92-e672530d5260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "b5laDFCgqUeV"
      },
      "outputs": [],
      "source": [
        "class DataLoader(Dataset):\n",
        "    def __init__(self, data_mat, labels):\n",
        "        self.data_mat = data_mat\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_mat.shape[2]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mat = np.array(self.data_mat[:,:,idx], dtype=np.float32)\n",
        "        mat_pad = np.zeros((43,13), dtype=np.float32)\n",
        "        mat_pad[1:42, 1:11] = mat\n",
        "        adj_mat = np.expand_dims(mat_pad, 0)\n",
        "        label = [self.labels[idx], np.abs(1-self.labels[idx])]\n",
        "        return torch.from_numpy(adj_mat), torch.from_numpy(np.array(label, dtype=np.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ynxfKuPsnMb3"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, numChannels=1, classes=2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layer1 = nn.Sequential(\n",
        "                            Conv2d(in_channels=numChannels, out_channels=32, kernel_size=(2,2)),\n",
        "                            ReLU(),\n",
        "                            MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "        \n",
        "        self.layer2 = nn.Sequential(\n",
        "                            Conv2d(in_channels=32, out_channels=64,kernel_size=(2,2)),\n",
        "                            ReLU(),\n",
        "                            MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "\n",
        "        self.flatten1 = Flatten(0,2)\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "                            Linear(in_features=1280, out_features=128),\n",
        "                            ReLU())\n",
        "\n",
        "        self.dropout = Dropout(p=0.2)\n",
        "\n",
        "        self.fc2 = nn.Sequential(\n",
        "                            Linear(in_features=128, out_features=classes),\n",
        "                            Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "      # print(\"x: \", x.shape)\n",
        "      x1 = self.layer1(x)\n",
        "      # print(\"x1:\", x1.shape)\n",
        "      x2 = self.layer2(x1)\n",
        "      # print(\"x2:\", x2.shape)\n",
        "      f1 = self.flatten1(x2)\n",
        "      # print(\"f1:\", f1.shape)\n",
        "      f2 = self.fc1(f1)\n",
        "      # print(\"f2:\", f2.shape)\n",
        "      f3 = self.dropout(f2)\n",
        "      # print(\"f3:\", f3.shape)\n",
        "      out = self.fc2(f3)\n",
        "      # print(\"out:\", out.shape)\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "mfSoxnwfqJNd"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = dataloader.__len__\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        #print(\"pred: \", pred, \"y: \", y)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss = loss.item()\n",
        "        #print(f\"loss: {loss:>7f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "aGK7Pd0jqL7X"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    tn = 0\n",
        "    fn = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            out = pred\n",
        "            if(y[0] == 1):\n",
        "              if(out[0] >= out[1]):\n",
        "                tp += 1\n",
        "              elif(out[0] < out[1]):\n",
        "                #print(out)\n",
        "                fn += 1\n",
        "            elif(y[0] == 0):\n",
        "              if(out[0] >= out[1]):\n",
        "                fp += 1\n",
        "              elif(out[0] < out[1]):\n",
        "                tn += 1\n",
        "    #print(test_loss, num_batches)\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    #print(f\"Test Error: \\n Metrics: {[tp, fp, tn, fn]}, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return [tp, fp, tn, fn], test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (41, 10, 395290)\n",
            "Labels shape: (395290,)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Directory containing .mat files\n",
        "folder_path = 'TFA/Train/'\n",
        "\n",
        "# List all files in the directory\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "# Filter out .mat files\n",
        "\n",
        "\n",
        "mat_files = [file for file in files if file.endswith('.mat')]\n",
        "\n",
        "dfs_train = []\n",
        "train_labels = []\n",
        "\n",
        "for file in mat_files:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    mat_data = scipy.io.loadmat(file_path)\n",
        "    data = mat_data[\"tfaOut\"]\n",
        "    data = np.transpose(data, (1, 0, 2))\n",
        "    \n",
        "    # Creating labels\n",
        "    label_column = np.zeros(len(data))\n",
        "    label_column[:5000] = 0\n",
        "    label_column[-5000:] = 0\n",
        "    label_column[5000:-5000] = 1\n",
        "    \n",
        "    # Append data and labels to lists\n",
        "    dfs_train.append(data)\n",
        "    train_labels.append(label_column)\n",
        "\n",
        "# Concatenate the lists into single numpy arrays\n",
        "dfs_train = np.concatenate(dfs_train, axis=0)\n",
        "train_labels = np.concatenate(train_labels, axis=0)\n",
        "dfs_train = np.transpose(dfs_train, (1, 2, 0))\n",
        "print(\"Data shape:\", dfs_train.shape)\n",
        "print(\"Labels shape:\", train_labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (41, 10, 47574)\n",
            "Labels shape: (47574,)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Directory containing .mat files\n",
        "folder_path = 'TFA/Test/'\n",
        "\n",
        "# List all files in the directory\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "# Filter out .mat files\n",
        "mat_files = [file for file in files if file.endswith('.mat')]\n",
        "\n",
        "dfs_test = []\n",
        "test_labels = []\n",
        "\n",
        "for file in mat_files:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    mat_data = scipy.io.loadmat(file_path)\n",
        "    data = mat_data[\"tfaOut\"]\n",
        "    data = np.transpose(data, (1, 0, 2))\n",
        "    \n",
        "    # Creating labels\n",
        "    label_column = np.zeros(len(data))\n",
        "    label_column[:5000] = 0\n",
        "    label_column[-5000:] = 0\n",
        "    label_column[5000:-5000] = 1\n",
        "    \n",
        "    # Append data and labels to lists\n",
        "    dfs_test.append(data)\n",
        "    test_labels.append(label_column)\n",
        "\n",
        "# Concatenate the lists into single numpy arrays\n",
        "dfs_test = np.concatenate(dfs_test, axis=0)\n",
        "test_labels = np.concatenate(test_labels, axis=0)\n",
        "dfs_test = np.transpose(dfs_test, (1, 2, 0))\n",
        "print(\"Data shape:\", dfs_test.shape)\n",
        "print(\"Labels shape:\", test_labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[4.46432128e-06, 3.02832006e-04, 5.71374953e-07, ...,\n",
              "         3.76182902e-05, 1.09440838e-05, 4.72718120e-04],\n",
              "        [2.08653004e-06, 1.41537322e-04, 2.67048657e-07, ...,\n",
              "         1.75819991e-05, 5.11503501e-06, 2.20938525e-04],\n",
              "        [9.54753126e-07, 6.47645604e-05, 1.22195960e-07, ...,\n",
              "         8.04516027e-06, 2.34053456e-06, 1.01096914e-04],\n",
              "        ...,\n",
              "        [5.70243174e-25, 3.86817780e-23, 7.29836959e-26, ...,\n",
              "         4.80511412e-24, 1.39792562e-24, 6.03819180e-23],\n",
              "        [1.21697802e-25, 8.25522793e-24, 1.55757330e-26, ...,\n",
              "         1.02547800e-24, 2.98336716e-25, 1.28863388e-23],\n",
              "        [2.54495614e-26, 1.72634119e-24, 3.25721224e-27, ...,\n",
              "         2.14448944e-25, 6.23884607e-26, 2.69480354e-24]],\n",
              "\n",
              "       [[5.74683736e-06, 2.17666275e-04, 9.58984523e-07, ...,\n",
              "         1.15746436e-04, 1.16334851e-05, 1.59876648e-04],\n",
              "        [2.68362615e-06, 1.01644586e-04, 4.47821259e-07, ...,\n",
              "         5.40506268e-05, 5.43254018e-06, 7.46583081e-05],\n",
              "        [1.22697242e-06, 4.64726074e-05, 2.04746975e-07, ...,\n",
              "         2.47123201e-05, 2.48379492e-06, 3.41342944e-05],\n",
              "        ...,\n",
              "        [7.01907484e-25, 2.65853334e-23, 1.17128495e-25, ...,\n",
              "         1.41370434e-23, 1.42089114e-24, 1.95270214e-23],\n",
              "        [1.49562905e-25, 5.66482021e-24, 2.49578163e-26, ...,\n",
              "         3.01233045e-24, 3.02764412e-25, 4.16083047e-24],\n",
              "        [3.12272628e-26, 1.18275871e-24, 5.21094645e-27, ...,\n",
              "         6.28944955e-25, 6.32142298e-26, 8.68740457e-25]],\n",
              "\n",
              "       [[6.07432284e-06, 1.37049548e-04, 1.26744405e-06, ...,\n",
              "         2.07771284e-04, 1.14765550e-05, 1.55771881e-05],\n",
              "        [2.82903117e-06, 6.38289158e-05, 5.90294394e-07, ...,\n",
              "         9.67665782e-05, 5.34504547e-06, 7.25485813e-06],\n",
              "        [1.29022308e-06, 2.91101565e-05, 2.69212817e-07, ...,\n",
              "         4.41318830e-05, 2.43769001e-06, 3.30868938e-06],\n",
              "        ...,\n",
              "        [6.43895390e-25, 1.45276394e-23, 1.34352652e-25, ...,\n",
              "         2.20243433e-23, 1.21654727e-24, 1.65122596e-24],\n",
              "        [1.36502361e-25, 3.07978144e-24, 2.84820398e-26, ...,\n",
              "         4.66904236e-24, 2.57901480e-25, 3.50051026e-25],\n",
              "        [2.83528417e-26, 6.39699967e-25, 5.91599121e-27, ...,\n",
              "         9.69804613e-25, 5.35685962e-26, 7.27089354e-26]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[1.95813605e-05, 1.00172857e-05, 1.46687552e-04, ...,\n",
              "         9.90192297e-06, 3.43499778e-06, 1.75636232e-07],\n",
              "        [9.08115277e-06, 4.64566807e-06, 6.80285759e-05, ...,\n",
              "         4.59216688e-06, 1.59303229e-06, 8.14539653e-08],\n",
              "        [4.12488227e-06, 2.11017636e-06, 3.09002473e-05, ...,\n",
              "         2.08587480e-06, 7.23594330e-07, 3.69983884e-08],\n",
              "        ...,\n",
              "        [1.59280402e-24, 8.14834745e-25, 1.19319862e-23, ...,\n",
              "         8.05450813e-25, 2.79412571e-25, 1.42867549e-26],\n",
              "        [3.33978915e-25, 1.70854430e-25, 2.50189712e-24, ...,\n",
              "         1.68886809e-25, 5.85871872e-26, 2.99564468e-27],\n",
              "        [6.85962490e-26, 3.50919550e-26, 5.13867044e-25, ...,\n",
              "         3.46878233e-26, 1.20332784e-26, 6.15278328e-28]],\n",
              "\n",
              "       [[1.76662558e-05, 1.40068516e-05, 1.81165473e-04, ...,\n",
              "         2.33945602e-05, 2.34590252e-06, 1.12106572e-06],\n",
              "        [8.22919431e-06, 6.52459157e-06, 8.43894655e-05, ...,\n",
              "         1.08975203e-05, 1.09275491e-06, 5.22208428e-07],\n",
              "        [3.75357498e-06, 2.97605607e-06, 3.84924908e-05, ...,\n",
              "         4.97067612e-06, 4.98437310e-07, 2.38194460e-07],\n",
              "        ...,\n",
              "        [1.87283322e-24, 1.48489285e-24, 1.92056948e-23, ...,\n",
              "         2.48010161e-24, 2.48693567e-25, 1.18846299e-25],\n",
              "        [3.96927969e-25, 3.14707951e-25, 4.07045184e-24, ...,\n",
              "         5.25632334e-25, 5.27080743e-26, 2.51882655e-26],\n",
              "        [8.24237724e-26, 6.53504377e-26, 8.45246550e-25, ...,\n",
              "         1.09149778e-25, 1.09450547e-26, 5.23044991e-27]],\n",
              "\n",
              "       [[1.40446428e-05, 1.70084397e-05, 2.03327163e-04, ...,\n",
              "         4.02688864e-05, 1.26672503e-06, 2.71427644e-06],\n",
              "        [6.55876017e-06, 7.94283478e-06, 9.49525111e-05, ...,\n",
              "         1.88053176e-05, 5.91552652e-07, 1.26755009e-06],\n",
              "        [2.99881342e-06, 3.63164362e-06, 4.34144346e-05, ...,\n",
              "         8.59821634e-06, 2.70471246e-07, 5.79552556e-07],\n",
              "        ...,\n",
              "        [1.71542273e-24, 2.07742302e-24, 2.48345254e-23, ...,\n",
              "         4.91847065e-24, 1.54718703e-25, 3.31523670e-25],\n",
              "        [3.65501305e-25, 4.42631900e-25, 5.29143707e-24, ...,\n",
              "         1.04796760e-24, 3.29655698e-26, 7.06370106e-26],\n",
              "        [7.63083409e-26, 9.24114510e-26, 1.10473144e-24, ...,\n",
              "         2.18791746e-25, 6.88245953e-27, 1.47473977e-26]]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfs_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3WtJGpD3XE1",
        "outputId": "bf33e6fe-069b-4756-ac59-77ea47f54fa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10, 11, 2194)\n",
            "=====================================\n",
            "Shape of concatenated training data: (10, 11, 2194)\n",
            "Shape of concatenated training labels: (2194,)\n",
            "Shape of concatenated testing data: (10, 11, 243)\n",
            "Shape of concatenated testing labels: (243,)\n"
          ]
        }
      ],
      "source": [
        "# # mat = scipy.io.loadmat('dataset/theta_A_W_uni11_100ms_080.mat')[\"W\"]\n",
        "\n",
        "# # # Calculate the number of samples in the data\n",
        "# # num_samples = mat.shape[2]\n",
        "\n",
        "# # # Labels initialization\n",
        "# # labels = np.zeros(num_samples)\n",
        "\n",
        "# # # Set first 250 and last 250 labels to 0\n",
        "# # labels[:250] = 0\n",
        "# # labels[-250:] = 0\n",
        "\n",
        "# # # Calculate the number of '1's for the middle part\n",
        "# # num_ones_middle = num_samples - 500\n",
        "\n",
        "# # # Set the middle part labels to 1\n",
        "# # labels[250:250 + num_ones_middle] = 1\n",
        "\n",
        "# # # Indices setup\n",
        "# # a1 = np.arange(0, 250)\n",
        "# # a2 = np.arange(250, 250 + num_ones_middle)\n",
        "# # a3 = np.arange(250 + num_ones_middle, num_samples)\n",
        "\n",
        "\n",
        "# # t1 = list(np.random.choice(a1, int(len(a1)*0.1), replace=False))\n",
        "# # tr1 = list(set(a1)-set(t1))\n",
        "# # t2 = list(np.random.choice(a2, int(len(a2)*0.1), replace=False))\n",
        "# # tr2 = list(set(a2)-set(t2))\n",
        "# # t3 = list(np.random.choice(a3, int(len(a3)*0.1), replace=False))\n",
        "# # tr3 = list(set(a3)-set(t3))\n",
        "\n",
        "# # t = t1 + t2 + t3\n",
        "# # tr = tr1 + tr2 + tr3\n",
        "# # test_data = mat[:,:,t]\n",
        "# # train_data = mat[:,:,tr]\n",
        "# # test_labels = labels[t]\n",
        "# # train_labels = labels[tr]\n",
        "\n",
        "# # print(train_data.shape)\n",
        "\n",
        "# # print(len(t), len(tr))\n",
        "\n",
        "# import os\n",
        "# import numpy as np\n",
        "# import scipy.io\n",
        "# import networkx as nx\n",
        "\n",
        "# # function to calculate various centrality measures of a graph with given adjacency matrix\n",
        "# def calculate_centrality_measures(adjacency_matrix):\n",
        "#     # Convert adjacency matrix to a networkx graph\n",
        "#     G = nx.from_numpy_array(adjacency_matrix)\n",
        "\n",
        "#     # # Calculate clustering coefficient for each node\n",
        "#     # clustering_coefficients = nx.clustering(G)\n",
        "\n",
        "#     # # Convert clustering coefficients to an array in the same order as nodes in the graph\n",
        "#     # coefficients_array = np.array([clustering_coefficients[node] for node in range(len(G))])\n",
        "\n",
        "#     # return coefficients_array\n",
        "\n",
        "#     # Calculate eigenvector centrality for each node\n",
        "#     eigenvector_centrality = nx.eigenvector_centrality_numpy(G)\n",
        "\n",
        "#     # Convert eigenvector centralities to an array in the same order as nodes in the graph\n",
        "#     centralities_array = np.array([eigenvector_centrality[node] for node in range(len(G))])\n",
        "\n",
        "#     return centralities_array\n",
        "\n",
        "# # Path to the folder containing the files\n",
        "# folder_path = 'dataset'\n",
        "\n",
        "# # Lists to append the data and labels\n",
        "# train_data_all = []\n",
        "# train_labels_all = []\n",
        "# test_data_all = []\n",
        "# test_labels_all = []\n",
        "\n",
        "# # List all files in the folder\n",
        "# file_names = os.listdir(folder_path)\n",
        "\n",
        "# # Loop through only one file\n",
        "# for file_name in file_names:\n",
        "#     # Load data from each file\n",
        "#     mat1 = scipy.io.loadmat(os.path.join(folder_path, file_name))[\"W\"]\n",
        "\n",
        "#     mat = np.zeros((10, 11, mat1.shape[2]))\n",
        "#     # append the clustering coefficient of the graph to the adjacency matrix\n",
        "#     for i in range(mat.shape[2]):\n",
        "#         # if i == 0:\n",
        "#         #     print(mat1[:,:,i].shape)\n",
        "#         #     print(calculate_centrality_measures(mat1[:,:,i]).flatten().shape)\n",
        "#         clustering_coefficients = calculate_centrality_measures(mat1[:,:,i]).flatten()\n",
        "#         # convert (48,48) adjacency matrix to (48,49) matrix\n",
        "#         mat[:,:,i] = np.concatenate((mat1[:,:,i], clustering_coefficients.reshape(10,1)), axis=1)\n",
        "#         # if i == 0:\n",
        "#         #     print(mat[:,:,i].shape)\n",
        "\n",
        "    \n",
        "#     # Calculate the number of samples in the data\n",
        "#     num_samples = mat.shape[2]\n",
        "    \n",
        "#     # Labels initialization\n",
        "#     labels = np.zeros(num_samples)\n",
        "    \n",
        "#     # Set first 250 and last 250 labels to 0\n",
        "#     labels[:250] = 0\n",
        "#     labels[-250:] = 0\n",
        "    \n",
        "#     # Calculate the number of '1's for the middle part\n",
        "#     num_ones_middle = num_samples - 500\n",
        "    \n",
        "#     # Set the middle part labels to 1\n",
        "#     labels[250:250 + num_ones_middle] = 1\n",
        "    \n",
        "#     # Randomly shuffle the labels\n",
        "#     np.random.shuffle(labels)\n",
        "    \n",
        "#     # Indices setup\n",
        "#     a1 = np.arange(0, 250)\n",
        "#     a2 = np.arange(250, 250 + num_ones_middle)\n",
        "#     a3 = np.arange(250 + num_ones_middle, num_samples)\n",
        "\n",
        "#     # Splitting indices for test and train sets\n",
        "#     t1 = list(np.random.choice(a1, int(len(a1) * 0.1), replace=False))\n",
        "#     tr1 = list(set(a1) - set(t1))\n",
        "#     t2 = list(np.random.choice(a2, int(len(a2) * 0.1), replace=False))\n",
        "#     tr2 = list(set(a2) - set(t2))\n",
        "#     t3 = list(np.random.choice(a3, int(len(a3) * 0.1), replace=False))\n",
        "#     tr3 = list(set(a3) - set(t3))\n",
        "    \n",
        "#     # Combine indices\n",
        "#     t = t1 + t2 + t3\n",
        "#     tr = tr1 + tr2 + tr3\n",
        "    \n",
        "#     # Split data\n",
        "#     test_data = mat[:, :, t]\n",
        "#     train_data = mat[:, :, tr]\n",
        "#     train_labels = labels[tr]\n",
        "#     test_labels = labels[t]\n",
        "\n",
        "#     print(train_data.shape)\n",
        "\n",
        "#     # Append the data and labels to the lists\n",
        "#     train_data_all.append(train_data)\n",
        "#     train_labels_all.append(train_labels)\n",
        "#     test_data_all.append(test_data)\n",
        "#     test_labels_all.append(test_labels)\n",
        "#     break\n",
        "\n",
        "\n",
        "# print(\"=====================================\")\n",
        "# # Find the maximum number of samples among accumulated data\n",
        "# max_samples = max(data.shape[2] for data in train_data_all)\n",
        "\n",
        "# # # Pad zeros at the end of each array to make them all 48x48x(max_samples)\n",
        "# # train_data_padded = []\n",
        "# # test_data_padded = []\n",
        "# # for data in train_data_all:\n",
        "# #     size = 48 - data.shape[0]\n",
        "# #     size//=2\n",
        "# #     padded_data = np.pad(data, ((size, size), (size, size), (0, 0)), mode='constant')\n",
        "# #     train_data_padded.append(padded_data)\n",
        "# #     print(padded_data.shape)\n",
        "# # for data in test_data_all:\n",
        "# #     size = 48 - data.shape[0]\n",
        "# #     size//=2\n",
        "# #     padded_data = np.pad(data, ((size, size), (size, size), (0, 0)), mode='constant')\n",
        "# #     test_data_padded.append(padded_data)\n",
        "\n",
        "# # Concatenate accumulated data and labels\n",
        "# train_data_concatenated = np.concatenate(train_data_all, axis=2)\n",
        "# train_labels_concatenated = np.concatenate(train_labels_all, axis=0)\n",
        "# test_data_concatenated = np.concatenate(test_data_all, axis=2)\n",
        "# test_labels_concatenated = np.concatenate(test_labels_all, axis=0)\n",
        "\n",
        "# # Print the shape of concatenated data\n",
        "# print(\"Shape of concatenated training data:\", train_data_concatenated.shape)\n",
        "# print(\"Shape of concatenated training labels:\", train_labels_concatenated.shape)\n",
        "# print(\"Shape of concatenated testing data:\", test_data_concatenated.shape)\n",
        "# print(\"Shape of concatenated testing labels:\", test_labels_concatenated.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyaVy6S2_25m",
        "outputId": "429a1561-1b77-49fb-ae2a-b215bfe22056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (flatten1): Flatten(start_dim=0, end_dim=2)\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=1280, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=2, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = Model().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22_b33vSqPDe",
        "outputId": "8ebd81e9-1bc9-46c8-e528-728ffc89b712"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training Loss: 1.627036329504261\n",
            "acc:  0.5796023037793753 pre:  0.5796023037793753 rec:  1.0 spe:  0.0 f_score:  0.7338585191888007 c:  [27574, 20000, 0, 0]\n",
            "Testing Loss: 2.9018201285639633\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(dfs_train, train_labels)\n",
        "test_dataloader = DataLoader(dfs_test, test_labels)\n",
        "model = Model().to(device)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [20], gamma=0.001)\n",
        "\n",
        "epochs = 1\n",
        "max_c = -1\n",
        "max_c_epoch = -1\n",
        "min_tl = 9999\n",
        "min_tl_epoch = 9999\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    model.train()  # set the model to train mode\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    for i, (data, labels) in enumerate(train_dataloader):\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()  # clear gradients\n",
        "        \n",
        "        outputs = model(data)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()  # backpropagation\n",
        "        optimizer.step()  # update weights\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    average_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Training Loss: {average_train_loss}\")\n",
        "    \n",
        "    # testing\n",
        "    model.eval()  # set the model to evaluation mode\n",
        "    c, tl = test(test_dataloader, model, loss_fn)\n",
        "    acc = (c[0] + c[2])/np.sum(c)\n",
        "    pre = c[0]/(c[0]+c[1])\n",
        "    sen = c[0]/(c[0]+c[3])\n",
        "    spe = c[2]/(c[1]+c[2])\n",
        "    f_score = 2*(pre*sen)/(pre+sen)\n",
        "    \n",
        "    print(\"acc: \", acc, \"pre: \", pre, \"rec: \", sen, \"spe: \", spe, \"f_score: \", f_score, \"c: \", c)\n",
        "    print(f\"Testing Loss: {tl}\")\n",
        "    \n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
