{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Directory containing .mat files\n",
    "folder_path = 'TFA/Train/'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Filter out .mat files\n",
    "mat_files = [file for file in files if file.endswith('.mat')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data into train and test as follows: 8 persons data for training and 2 persons data for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 20258, 10)\n",
      "(20258, 410)\n",
      "(41, 89167, 10)\n",
      "(89167, 410)\n",
      "(41, 85351, 10)\n",
      "(85351, 410)\n",
      "(41, 18127, 10)\n",
      "(18127, 410)\n",
      "(41, 21829, 10)\n",
      "(21829, 410)\n",
      "(41, 77912, 10)\n",
      "(77912, 410)\n",
      "(41, 33906, 10)\n",
      "(33906, 410)\n",
      "(41, 48740, 10)\n",
      "(48740, 410)\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "for file in mat_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    mat_data = scipy.io.loadmat(file_path)\n",
    "    # Assuming your data is stored in 'tfaOut' key\n",
    "    data = mat_data[\"tfaOut\"]\n",
    "    print(np.shape(data))\n",
    "    # Transpose data to swap the dimensions\n",
    "    transposed_data = np.transpose(data, (1, 0, 2))\n",
    "    # Reshape transposed data to 2D array\n",
    "    shape = transposed_data.shape\n",
    "    reshaped_data = transposed_data.reshape(shape[0], -1)\n",
    "    df = pd.DataFrame(reshaped_data)\n",
    "    print(np.shape(df))\n",
    "    label_column = np.zeros(len(df))  # Initialize with zeros\n",
    "    label_column[:5000] = 0 # First 5000 rows get value 0\n",
    "    label_column[-5000:] = 0  # Last 5000 rows get value 0\n",
    "    label_column[5000:-5000] = 1\n",
    "    df['label'] = label_column \n",
    "    dfs.append(df)\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df_train = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>7.008969e-07</td>\n",
       "      <td>9.499883e-05</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>1.100189e-25</td>\n",
       "      <td>1.433261e-25</td>\n",
       "      <td>3.995573e-27</td>\n",
       "      <td>5.415557e-25</td>\n",
       "      <td>9.486691e-24</td>\n",
       "      <td>1.544730e-25</td>\n",
       "      <td>1.997394e-25</td>\n",
       "      <td>3.277730e-24</td>\n",
       "      <td>3.037160e-25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>4.948846e-07</td>\n",
       "      <td>2.699616e-05</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>...</td>\n",
       "      <td>5.432920e-25</td>\n",
       "      <td>1.477424e-25</td>\n",
       "      <td>2.689030e-27</td>\n",
       "      <td>1.466876e-25</td>\n",
       "      <td>1.184130e-23</td>\n",
       "      <td>7.653895e-25</td>\n",
       "      <td>7.316077e-25</td>\n",
       "      <td>4.244378e-24</td>\n",
       "      <td>6.611510e-25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>3.986104e-06</td>\n",
       "      <td>5.100606e-07</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>...</td>\n",
       "      <td>1.037810e-24</td>\n",
       "      <td>1.134149e-25</td>\n",
       "      <td>1.860307e-26</td>\n",
       "      <td>2.380443e-27</td>\n",
       "      <td>1.183297e-23</td>\n",
       "      <td>1.439358e-24</td>\n",
       "      <td>1.276510e-24</td>\n",
       "      <td>4.370623e-24</td>\n",
       "      <td>9.056497e-25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>8.104923e-06</td>\n",
       "      <td>1.082350e-05</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>...</td>\n",
       "      <td>1.222853e-24</td>\n",
       "      <td>5.901307e-26</td>\n",
       "      <td>2.841358e-26</td>\n",
       "      <td>3.794415e-26</td>\n",
       "      <td>9.293149e-24</td>\n",
       "      <td>1.674689e-24</td>\n",
       "      <td>1.440337e-24</td>\n",
       "      <td>3.517460e-24</td>\n",
       "      <td>8.697190e-25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>9.835971e-06</td>\n",
       "      <td>4.195622e-05</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>...</td>\n",
       "      <td>9.375238e-25</td>\n",
       "      <td>1.626744e-26</td>\n",
       "      <td>2.098587e-26</td>\n",
       "      <td>8.951713e-26</td>\n",
       "      <td>5.334137e-24</td>\n",
       "      <td>1.277743e-24</td>\n",
       "      <td>1.077082e-24</td>\n",
       "      <td>2.057989e-24</td>\n",
       "      <td>5.717150e-25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395285</th>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>5.806670e-04</td>\n",
       "      <td>6.472682e-04</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>...</td>\n",
       "      <td>2.594366e-25</td>\n",
       "      <td>3.622873e-25</td>\n",
       "      <td>4.837371e-25</td>\n",
       "      <td>5.392206e-25</td>\n",
       "      <td>7.380861e-25</td>\n",
       "      <td>3.964984e-25</td>\n",
       "      <td>5.117189e-25</td>\n",
       "      <td>8.026859e-25</td>\n",
       "      <td>1.184336e-25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395286</th>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>7.006664e-04</td>\n",
       "      <td>8.236793e-04</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>...</td>\n",
       "      <td>5.768042e-25</td>\n",
       "      <td>1.140592e-24</td>\n",
       "      <td>1.494167e-24</td>\n",
       "      <td>1.756492e-24</td>\n",
       "      <td>2.263834e-24</td>\n",
       "      <td>1.739039e-24</td>\n",
       "      <td>1.665312e-24</td>\n",
       "      <td>2.640725e-24</td>\n",
       "      <td>2.576752e-25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395287</th>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>7.519064e-04</td>\n",
       "      <td>9.366418e-04</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>...</td>\n",
       "      <td>6.734301e-25</td>\n",
       "      <td>2.039456e-24</td>\n",
       "      <td>2.635312e-24</td>\n",
       "      <td>3.282780e-24</td>\n",
       "      <td>3.981134e-24</td>\n",
       "      <td>4.266449e-24</td>\n",
       "      <td>3.101012e-24</td>\n",
       "      <td>4.944687e-24</td>\n",
       "      <td>2.917922e-25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395288</th>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>7.200209e-04</td>\n",
       "      <td>9.593292e-04</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>4.761704e-25</td>\n",
       "      <td>2.623168e-24</td>\n",
       "      <td>3.359977e-24</td>\n",
       "      <td>4.476709e-24</td>\n",
       "      <td>5.076395e-24</td>\n",
       "      <td>7.609251e-24</td>\n",
       "      <td>4.204713e-24</td>\n",
       "      <td>6.700812e-24</td>\n",
       "      <td>1.985795e-25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395289</th>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>6.127607e-04</td>\n",
       "      <td>8.868214e-04</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>1.651055e-25</td>\n",
       "      <td>2.608123e-24</td>\n",
       "      <td>3.329444e-24</td>\n",
       "      <td>4.818557e-24</td>\n",
       "      <td>5.041570e-24</td>\n",
       "      <td>1.083815e-23</td>\n",
       "      <td>4.492633e-24</td>\n",
       "      <td>7.098912e-24</td>\n",
       "      <td>6.602647e-26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>395290 rows × 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2             3             4         5  \\\n",
       "0       0.000035  0.000019  0.000025  7.008969e-07  9.499883e-05  0.001664   \n",
       "1       0.000029  0.000100  0.000027  4.948846e-07  2.699616e-05  0.002179   \n",
       "2       0.000019  0.000222  0.000024  3.986104e-06  5.100606e-07  0.002535   \n",
       "3       0.000010  0.000349  0.000017  8.104923e-06  1.082350e-05  0.002651   \n",
       "4       0.000004  0.000439  0.000008  9.835971e-06  4.195622e-05  0.002500   \n",
       "...          ...       ...       ...           ...           ...       ...   \n",
       "395285  0.000520  0.000311  0.000435  5.806670e-04  6.472682e-04  0.000886   \n",
       "395286  0.000575  0.000270  0.000535  7.006664e-04  8.236793e-04  0.001062   \n",
       "395287  0.000561  0.000192  0.000582  7.519064e-04  9.366418e-04  0.001136   \n",
       "395288  0.000481  0.000102  0.000562  7.200209e-04  9.593292e-04  0.001088   \n",
       "395289  0.000356  0.000030  0.000480  6.127607e-04  8.868214e-04  0.000928   \n",
       "\n",
       "               6         7         8         9  ...           401  \\\n",
       "0       0.000027  0.000035  0.000575  0.000053  ...  1.100189e-25   \n",
       "1       0.000141  0.000135  0.000781  0.000122  ...  5.432920e-25   \n",
       "2       0.000308  0.000274  0.000936  0.000194  ...  1.037810e-24   \n",
       "3       0.000478  0.000411  0.001003  0.000248  ...  1.222853e-24   \n",
       "4       0.000599  0.000505  0.000965  0.000268  ...  9.375238e-25   \n",
       "...          ...       ...       ...       ...  ...           ...   \n",
       "395285  0.000476  0.000614  0.000964  0.000142  ...  2.594366e-25   \n",
       "395286  0.000815  0.000781  0.001238  0.000121  ...  5.768042e-25   \n",
       "395287  0.001217  0.000885  0.001411  0.000083  ...  6.734301e-25   \n",
       "395288  0.001631  0.000901  0.001436  0.000043  ...  4.761704e-25   \n",
       "395289  0.001995  0.000827  0.001307  0.000012  ...  1.651055e-25   \n",
       "\n",
       "                 402           403           404           405           406  \\\n",
       "0       1.433261e-25  3.995573e-27  5.415557e-25  9.486691e-24  1.544730e-25   \n",
       "1       1.477424e-25  2.689030e-27  1.466876e-25  1.184130e-23  7.653895e-25   \n",
       "2       1.134149e-25  1.860307e-26  2.380443e-27  1.183297e-23  1.439358e-24   \n",
       "3       5.901307e-26  2.841358e-26  3.794415e-26  9.293149e-24  1.674689e-24   \n",
       "4       1.626744e-26  2.098587e-26  8.951713e-26  5.334137e-24  1.277743e-24   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "395285  3.622873e-25  4.837371e-25  5.392206e-25  7.380861e-25  3.964984e-25   \n",
       "395286  1.140592e-24  1.494167e-24  1.756492e-24  2.263834e-24  1.739039e-24   \n",
       "395287  2.039456e-24  2.635312e-24  3.282780e-24  3.981134e-24  4.266449e-24   \n",
       "395288  2.623168e-24  3.359977e-24  4.476709e-24  5.076395e-24  7.609251e-24   \n",
       "395289  2.608123e-24  3.329444e-24  4.818557e-24  5.041570e-24  1.083815e-23   \n",
       "\n",
       "                 407           408           409  label  \n",
       "0       1.997394e-25  3.277730e-24  3.037160e-25    0.0  \n",
       "1       7.316077e-25  4.244378e-24  6.611510e-25    0.0  \n",
       "2       1.276510e-24  4.370623e-24  9.056497e-25    0.0  \n",
       "3       1.440337e-24  3.517460e-24  8.697190e-25    0.0  \n",
       "4       1.077082e-24  2.057989e-24  5.717150e-25    0.0  \n",
       "...              ...           ...           ...    ...  \n",
       "395285  5.117189e-25  8.026859e-25  1.184336e-25    0.0  \n",
       "395286  1.665312e-24  2.640725e-24  2.576752e-25    0.0  \n",
       "395287  3.101012e-24  4.944687e-24  2.917922e-25    0.0  \n",
       "395288  4.204713e-24  6.700812e-24  1.985795e-25    0.0  \n",
       "395289  4.492633e-24  7.098912e-24  6.602647e-26    0.0  \n",
       "\n",
       "[395290 rows x 411 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 22249, 10)\n",
      "(22249, 410)\n",
      "(41, 25325, 10)\n",
      "(25325, 410)\n"
     ]
    }
   ],
   "source": [
    "# Directory containing .mat files\n",
    "folder_path = 'TFA/Test/'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Filter out .mat files\n",
    "mat_files = [file for file in files if file.endswith('.mat')]\n",
    "# Load each .mat file\n",
    "dfs = []\n",
    "\n",
    "for file in mat_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    mat_data = scipy.io.loadmat(file_path)\n",
    "    # Assuming your data is stored in 'tfaOut' key\n",
    "    data = mat_data[\"tfaOut\"]\n",
    "    print(np.shape(data))\n",
    "    # Transpose data to swap the dimensions\n",
    "    transposed_data = np.transpose(data, (1, 0, 2))\n",
    "    # Reshape transposed data to 2D array\n",
    "    shape = transposed_data.shape\n",
    "    reshaped_data = transposed_data.reshape(shape[0], -1)\n",
    "    df = pd.DataFrame(reshaped_data)\n",
    "    print(np.shape(df))\n",
    "    label_column = np.zeros(len(df))  # Initialize with zeros\n",
    "    label_column[:5000] = 0 # First 5000 rows get value 0\n",
    "    label_column[-5000:] = 0  # Last 5000 rows get value 0\n",
    "    label_column[5000:-5000] = 1\n",
    "    df['label'] = label_column \n",
    "    dfs.append(df)\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df_test = pd.concat(dfs, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>5.713750e-07</td>\n",
       "      <td>1.040541e-05</td>\n",
       "      <td>9.201673e-07</td>\n",
       "      <td>9.036281e-06</td>\n",
       "      <td>9.542928e-05</td>\n",
       "      <td>3.761829e-05</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>4.727181e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.726341e-24</td>\n",
       "      <td>3.257212e-27</td>\n",
       "      <td>5.931768e-26</td>\n",
       "      <td>5.245557e-27</td>\n",
       "      <td>5.151273e-26</td>\n",
       "      <td>5.440095e-25</td>\n",
       "      <td>2.144489e-25</td>\n",
       "      <td>6.238846e-26</td>\n",
       "      <td>2.694804e-24</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>9.589845e-07</td>\n",
       "      <td>1.131843e-05</td>\n",
       "      <td>7.895691e-07</td>\n",
       "      <td>8.519349e-06</td>\n",
       "      <td>8.149424e-05</td>\n",
       "      <td>1.157464e-04</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1.598766e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.182759e-24</td>\n",
       "      <td>5.210946e-27</td>\n",
       "      <td>6.150229e-26</td>\n",
       "      <td>4.290374e-27</td>\n",
       "      <td>4.629259e-26</td>\n",
       "      <td>4.428248e-25</td>\n",
       "      <td>6.289450e-25</td>\n",
       "      <td>6.321423e-26</td>\n",
       "      <td>8.687405e-25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>1.267444e-06</td>\n",
       "      <td>1.090369e-05</td>\n",
       "      <td>6.051599e-07</td>\n",
       "      <td>7.468137e-06</td>\n",
       "      <td>6.170625e-05</td>\n",
       "      <td>2.077713e-04</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.557719e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>6.397000e-25</td>\n",
       "      <td>5.915991e-27</td>\n",
       "      <td>5.089464e-26</td>\n",
       "      <td>2.824677e-27</td>\n",
       "      <td>3.485869e-26</td>\n",
       "      <td>2.880234e-25</td>\n",
       "      <td>9.698046e-25</td>\n",
       "      <td>5.356860e-26</td>\n",
       "      <td>7.270894e-26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>1.387696e-06</td>\n",
       "      <td>9.281459e-06</td>\n",
       "      <td>3.885006e-07</td>\n",
       "      <td>6.163789e-06</td>\n",
       "      <td>4.011502e-05</td>\n",
       "      <td>2.827980e-04</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.358481e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.545122e-25</td>\n",
       "      <td>4.866958e-27</td>\n",
       "      <td>3.255213e-26</td>\n",
       "      <td>1.362557e-27</td>\n",
       "      <td>2.161777e-26</td>\n",
       "      <td>1.406922e-25</td>\n",
       "      <td>9.918349e-25</td>\n",
       "      <td>3.719183e-26</td>\n",
       "      <td>4.764492e-26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>1.283622e-06</td>\n",
       "      <td>6.909502e-06</td>\n",
       "      <td>1.837786e-07</td>\n",
       "      <td>4.848268e-06</td>\n",
       "      <td>2.100184e-05</td>\n",
       "      <td>3.178725e-04</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>9.498834e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>6.318398e-26</td>\n",
       "      <td>2.741910e-27</td>\n",
       "      <td>1.475920e-26</td>\n",
       "      <td>3.925644e-28</td>\n",
       "      <td>1.035625e-26</td>\n",
       "      <td>4.486146e-26</td>\n",
       "      <td>6.789986e-25</td>\n",
       "      <td>1.971091e-26</td>\n",
       "      <td>2.029020e-25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47569</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>6.952939e-05</td>\n",
       "      <td>2.463632e-06</td>\n",
       "      <td>3.954706e-06</td>\n",
       "      <td>6.739955e-10</td>\n",
       "      <td>1.596383e-06</td>\n",
       "      <td>3.265827e-08</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>5.031395e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.375570e-27</td>\n",
       "      <td>5.777654e-26</td>\n",
       "      <td>2.047194e-27</td>\n",
       "      <td>3.286226e-27</td>\n",
       "      <td>5.600672e-31</td>\n",
       "      <td>1.326540e-27</td>\n",
       "      <td>2.713791e-29</td>\n",
       "      <td>4.000078e-27</td>\n",
       "      <td>4.180917e-28</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47570</th>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.071067e-04</td>\n",
       "      <td>1.387205e-06</td>\n",
       "      <td>2.486203e-06</td>\n",
       "      <td>4.907807e-08</td>\n",
       "      <td>1.226702e-06</td>\n",
       "      <td>2.026941e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>3.772047e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.284550e-26</td>\n",
       "      <td>2.281608e-25</td>\n",
       "      <td>2.955053e-27</td>\n",
       "      <td>5.296163e-27</td>\n",
       "      <td>1.045471e-28</td>\n",
       "      <td>2.613146e-27</td>\n",
       "      <td>4.317833e-27</td>\n",
       "      <td>9.190610e-27</td>\n",
       "      <td>8.035294e-29</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47571</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.466876e-04</td>\n",
       "      <td>4.450132e-07</td>\n",
       "      <td>1.044123e-06</td>\n",
       "      <td>1.712960e-07</td>\n",
       "      <td>7.219777e-07</td>\n",
       "      <td>9.901923e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.756362e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>3.509195e-26</td>\n",
       "      <td>5.138670e-25</td>\n",
       "      <td>1.558944e-27</td>\n",
       "      <td>3.657710e-27</td>\n",
       "      <td>6.000738e-28</td>\n",
       "      <td>2.529189e-27</td>\n",
       "      <td>3.468782e-26</td>\n",
       "      <td>1.203328e-26</td>\n",
       "      <td>6.152783e-28</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47572</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.811655e-04</td>\n",
       "      <td>5.676815e-09</td>\n",
       "      <td>1.342265e-07</td>\n",
       "      <td>3.434736e-07</td>\n",
       "      <td>2.673473e-07</td>\n",
       "      <td>2.339456e-05</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.121066e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>6.535044e-26</td>\n",
       "      <td>8.452465e-25</td>\n",
       "      <td>2.648578e-29</td>\n",
       "      <td>6.262479e-28</td>\n",
       "      <td>1.602512e-27</td>\n",
       "      <td>1.247337e-27</td>\n",
       "      <td>1.091498e-25</td>\n",
       "      <td>1.094505e-26</td>\n",
       "      <td>5.230450e-27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47573</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>2.033272e-04</td>\n",
       "      <td>3.057708e-07</td>\n",
       "      <td>1.143025e-07</td>\n",
       "      <td>5.247104e-07</td>\n",
       "      <td>2.159444e-08</td>\n",
       "      <td>4.026889e-05</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.714276e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>9.241145e-26</td>\n",
       "      <td>1.104731e-24</td>\n",
       "      <td>1.661335e-27</td>\n",
       "      <td>6.210363e-28</td>\n",
       "      <td>2.850894e-27</td>\n",
       "      <td>1.173284e-28</td>\n",
       "      <td>2.187917e-25</td>\n",
       "      <td>6.882460e-27</td>\n",
       "      <td>1.474740e-26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47574 rows × 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1             2             3             4  \\\n",
       "0      0.000004  0.000303  5.713750e-07  1.040541e-05  9.201673e-07   \n",
       "1      0.000006  0.000218  9.589845e-07  1.131843e-05  7.895691e-07   \n",
       "2      0.000006  0.000137  1.267444e-06  1.090369e-05  6.051599e-07   \n",
       "3      0.000005  0.000073  1.387696e-06  9.281459e-06  3.885006e-07   \n",
       "4      0.000004  0.000030  1.283622e-06  6.909502e-06  1.837786e-07   \n",
       "...         ...       ...           ...           ...           ...   \n",
       "47569  0.000017  0.000003  6.952939e-05  2.463632e-06  3.954706e-06   \n",
       "47570  0.000019  0.000006  1.071067e-04  1.387205e-06  2.486203e-06   \n",
       "47571  0.000020  0.000010  1.466876e-04  4.450132e-07  1.044123e-06   \n",
       "47572  0.000018  0.000014  1.811655e-04  5.676815e-09  1.342265e-07   \n",
       "47573  0.000014  0.000017  2.033272e-04  3.057708e-07  1.143025e-07   \n",
       "\n",
       "                  5             6             7         8             9  ...  \\\n",
       "0      9.036281e-06  9.542928e-05  3.761829e-05  0.000011  4.727181e-04  ...   \n",
       "1      8.519349e-06  8.149424e-05  1.157464e-04  0.000012  1.598766e-04  ...   \n",
       "2      7.468137e-06  6.170625e-05  2.077713e-04  0.000011  1.557719e-05  ...   \n",
       "3      6.163789e-06  4.011502e-05  2.827980e-04  0.000011  1.358481e-05  ...   \n",
       "4      4.848268e-06  2.100184e-05  3.178725e-04  0.000009  9.498834e-05  ...   \n",
       "...             ...           ...           ...       ...           ...  ...   \n",
       "47569  6.739955e-10  1.596383e-06  3.265827e-08  0.000005  5.031395e-07  ...   \n",
       "47570  4.907807e-08  1.226702e-06  2.026941e-06  0.000004  3.772047e-08  ...   \n",
       "47571  1.712960e-07  7.219777e-07  9.901923e-06  0.000003  1.756362e-07  ...   \n",
       "47572  3.434736e-07  2.673473e-07  2.339456e-05  0.000002  1.121066e-06  ...   \n",
       "47573  5.247104e-07  2.159444e-08  4.026889e-05  0.000001  2.714276e-06  ...   \n",
       "\n",
       "                401           402           403           404           405  \\\n",
       "0      1.726341e-24  3.257212e-27  5.931768e-26  5.245557e-27  5.151273e-26   \n",
       "1      1.182759e-24  5.210946e-27  6.150229e-26  4.290374e-27  4.629259e-26   \n",
       "2      6.397000e-25  5.915991e-27  5.089464e-26  2.824677e-27  3.485869e-26   \n",
       "3      2.545122e-25  4.866958e-27  3.255213e-26  1.362557e-27  2.161777e-26   \n",
       "4      6.318398e-26  2.741910e-27  1.475920e-26  3.925644e-28  1.035625e-26   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "47569  2.375570e-27  5.777654e-26  2.047194e-27  3.286226e-27  5.600672e-31   \n",
       "47570  1.284550e-26  2.281608e-25  2.955053e-27  5.296163e-27  1.045471e-28   \n",
       "47571  3.509195e-26  5.138670e-25  1.558944e-27  3.657710e-27  6.000738e-28   \n",
       "47572  6.535044e-26  8.452465e-25  2.648578e-29  6.262479e-28  1.602512e-27   \n",
       "47573  9.241145e-26  1.104731e-24  1.661335e-27  6.210363e-28  2.850894e-27   \n",
       "\n",
       "                406           407           408           409  label  \n",
       "0      5.440095e-25  2.144489e-25  6.238846e-26  2.694804e-24    0.0  \n",
       "1      4.428248e-25  6.289450e-25  6.321423e-26  8.687405e-25    0.0  \n",
       "2      2.880234e-25  9.698046e-25  5.356860e-26  7.270894e-26    0.0  \n",
       "3      1.406922e-25  9.918349e-25  3.719183e-26  4.764492e-26    0.0  \n",
       "4      4.486146e-26  6.789986e-25  1.971091e-26  2.029020e-25    0.0  \n",
       "...             ...           ...           ...           ...    ...  \n",
       "47569  1.326540e-27  2.713791e-29  4.000078e-27  4.180917e-28    0.0  \n",
       "47570  2.613146e-27  4.317833e-27  9.190610e-27  8.035294e-29    0.0  \n",
       "47571  2.529189e-27  3.468782e-26  1.203328e-26  6.152783e-28    0.0  \n",
       "47572  1.247337e-27  1.091498e-25  1.094505e-26  5.230450e-27    0.0  \n",
       "47573  1.173284e-28  2.187917e-25  6.882460e-27  1.474740e-26    0.0  \n",
       "\n",
       "[47574 rows x 411 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:06:43.078533: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-09 15:06:43.126851: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-09 15:06:43.126909: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-09 15:06:43.126965: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-09 15:06:43.137057: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-09 15:06:44.709387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def split_features_target(data):\n",
    "  # Assuming the label is in the last column\n",
    "  features = data[:, :-1]  # All columns except the last\n",
    "  target = data[:, -1]  # Last column\n",
    "  features = torch.tensor(features)\n",
    "  target = torch.tensor(target)\n",
    "  return features, target\n",
    "\n",
    "# Split features and target\n",
    "X_train, y_train = split_features_target(df_train.to_numpy())\n",
    "y_train = tf.expand_dims(y_train, axis=1)\n",
    "\n",
    "X_test,y_test = split_features_target(df_test.to_numpy())\n",
    "y_test = tf.expand_dims(y_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([395290, 410]) (395290, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape , y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 410, 1)            0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 408, 32)           128       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 204, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6528)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               835712    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 835,969\n",
      "Trainable params: 835,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:04:41.341002: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 648275600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8535/12353 [===================>..........] - ETA: 36s - loss: 0.4746 - accuracy: 0.7978"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m X_train_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(X_train)\n\u001b[1;32m     30\u001b[0m y_train_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(y_train)\n\u001b[0;32m---> 32\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train_np, y_train_np, epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1485\u001b[0m   )\n\u001b[1;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (410,)  # Assuming each sample has 10 features\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Reshape(input_shape + (1,), input_shape=input_shape),  # Reshape input to match (10, 1) for grayscale\n",
    "    layers.Conv1D(32, kernel_size=3, activation='relu'),           # Convolutional layer with 32 filters\n",
    "    layers.MaxPooling1D(pool_size=2),                              # Max pooling layer\n",
    "    layers.Flatten(),                                              # Flatten layer\n",
    "    layers.Dense(128, activation='relu'),                         # Dense layer with 128 neurons\n",
    "    layers.Dropout(0.5),                                           # Dropout layer with dropout rate of 0.5\n",
    "    layers.Dense(1, activation='sigmoid')                          # Output layer with 1 neuron and sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_train and y_train are your input and target data\n",
    "X_train_np = np.array(X_train)\n",
    "y_train_np = np.array(y_train)\n",
    "\n",
    "history = model.fit(X_train_np, y_train_np, epochs=2, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60955/60955 [==============================] - 61s 998us/step\n",
      "Precision: 0.9587043274821376\n",
      "Recall: 0.9993775869210836\n",
      "Confusion Matrix:\n",
      " [[   1564   80436]\n",
      " [   1163 1867371]]\n",
      "Accuracy: 0.9581658151049918\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "\n",
    "X_test_np = np.array(X_test)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# Assuming X_test_np is your test data\n",
    "X_test_np = np.array(X_test_np)\n",
    "\n",
    "# Predict probabilities for each class\n",
    "y_pred_prob = model.predict(X_test_np)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test_np, y_pred)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test_np, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_np, y_pred)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_np, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 128)               52608     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,929\n",
      "Trainable params: 60,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:07:43.872360: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 648275600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12353/12353 [==============================] - 20s 2ms/step - loss: 0.4648 - accuracy: 0.7976\n",
      "Epoch 2/20\n",
      "12353/12353 [==============================] - 21s 2ms/step - loss: 0.4585 - accuracy: 0.7976\n",
      "Epoch 3/20\n",
      "12353/12353 [==============================] - 21s 2ms/step - loss: 0.4569 - accuracy: 0.7976\n",
      "Epoch 4/20\n",
      "12353/12353 [==============================] - 20s 2ms/step - loss: 0.4541 - accuracy: 0.7976\n",
      "Epoch 5/20\n",
      "12353/12353 [==============================] - 21s 2ms/step - loss: 0.4520 - accuracy: 0.7976\n",
      "Epoch 6/20\n",
      "12353/12353 [==============================] - 21s 2ms/step - loss: 0.4507 - accuracy: 0.7976\n",
      "Epoch 7/20\n",
      "12353/12353 [==============================] - 20s 2ms/step - loss: 0.4499 - accuracy: 0.7976\n",
      "Epoch 8/20\n",
      "12353/12353 [==============================] - 20s 2ms/step - loss: 0.4493 - accuracy: 0.7976\n",
      "Epoch 9/20\n",
      "12353/12353 [==============================] - 20s 2ms/step - loss: 0.4485 - accuracy: 0.7976\n",
      "Epoch 10/20\n",
      "12353/12353 [==============================] - 21s 2ms/step - loss: 0.4479 - accuracy: 0.7976\n",
      "Epoch 11/20\n",
      "12353/12353 [==============================] - 20s 2ms/step - loss: 0.4473 - accuracy: 0.7976\n",
      "Epoch 12/20\n",
      "12353/12353 [==============================] - 21s 2ms/step - loss: 0.4464 - accuracy: 0.7976\n",
      "Epoch 13/20\n",
      "12353/12353 [==============================] - 22s 2ms/step - loss: 0.4459 - accuracy: 0.7976\n",
      "Epoch 14/20\n",
      "12353/12353 [==============================] - 21s 2ms/step - loss: 0.4454 - accuracy: 0.7976\n",
      "Epoch 15/20\n",
      "12353/12353 [==============================] - 21s 2ms/step - loss: 0.4447 - accuracy: 0.7976\n",
      "Epoch 16/20\n",
      "12353/12353 [==============================] - 22s 2ms/step - loss: 0.4439 - accuracy: 0.7976\n",
      "Epoch 17/20\n",
      "12353/12353 [==============================] - 21s 2ms/step - loss: 0.4434 - accuracy: 0.7976\n",
      "Epoch 18/20\n",
      "12353/12353 [==============================] - 21s 2ms/step - loss: 0.4430 - accuracy: 0.7976\n",
      "Epoch 19/20\n",
      "12353/12353 [==============================] - 22s 2ms/step - loss: 0.4424 - accuracy: 0.7976\n",
      "Epoch 20/20\n",
      "12353/12353 [==============================] - 21s 2ms/step - loss: 0.4422 - accuracy: 0.7976\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (410,)  # Assuming each sample has 10 features\n",
    "\n",
    "# Define the neural network model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=input_shape),  # Dense layer with 64 neurons and ReLU activation\n",
    "    layers.Dense(64, activation='relu'),                            # Dense layer with 32 neurons and ReLU activation\n",
    "    layers.Dropout(0.1),                                             # Dropout layer with dropout rate of 0.5\n",
    "    layers.Dense(1, activation='sigmoid')                            # Output layer with 1 neuron and sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_train and y_train are your input and target data\n",
    "X_train_np = np.array(X_train)\n",
    "y_train_np = np.array(y_train)\n",
    "\n",
    "history = model.fit(X_train_np, y_train_np, epochs=20, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1487/1487 [==============================] - 2s 1ms/step\n",
      "Precision: 0.5794520259898648\n",
      "Recall: 0.9993834771886559\n",
      "Confusion Matrix:\n",
      " [[    0 20000]\n",
      " [   17 27557]]\n",
      "Accuracy: 0.5792449657375878\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "\n",
    "X_test_np = np.array(X_test)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# Assuming X_test_np is your test data\n",
    "X_test_np = np.array(X_test_np)\n",
    "\n",
    "# Predict probabilities for each class\n",
    "y_pred_prob = model.predict(X_test_np)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test_np, y_pred)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test_np, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_np, y_pred)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_np, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
