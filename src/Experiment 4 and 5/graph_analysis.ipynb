{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","gpuClass":"standard","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8043888,"sourceType":"datasetVersion","datasetId":4742799},{"sourceId":8052739,"sourceType":"datasetVersion","datasetId":4749087},{"sourceId":8055980,"sourceType":"datasetVersion","datasetId":4751395},{"sourceId":8071019,"sourceType":"datasetVersion","datasetId":4762309},{"sourceId":8079883,"sourceType":"datasetVersion","datasetId":4768780},{"sourceId":8082707,"sourceType":"datasetVersion","datasetId":4770896}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport scipy.io\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn import Module\nfrom torch.nn import Conv2d\nfrom torch.nn import Linear\nfrom torch.nn import MaxPool2d\nfrom torch.nn import ReLU\nfrom torch.nn import LogSoftmax\nfrom torch.nn import Flatten\nfrom torch.nn import Flatten\nfrom torch.nn import Dropout\nfrom torch.nn import Sigmoid","metadata":{"id":"fTwy1ViNm71A","execution":{"iopub.status.busy":"2024-04-10T13:54:16.214468Z","iopub.execute_input":"2024-04-10T13:54:16.215117Z","iopub.status.idle":"2024-04-10T13:54:19.412031Z","shell.execute_reply.started":"2024-04-10T13:54:16.215085Z","shell.execute_reply":"2024-04-10T13:54:19.411176Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"device = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ig0sMeTMpbW1","outputId":"19e6a0c7-6c9e-4afe-fd92-e672530d5260","execution":{"iopub.status.busy":"2024-04-10T13:54:19.413674Z","iopub.execute_input":"2024-04-10T13:54:19.414142Z","iopub.status.idle":"2024-04-10T13:54:19.469689Z","shell.execute_reply.started":"2024-04-10T13:54:19.414116Z","shell.execute_reply":"2024-04-10T13:54:19.468618Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"}]},{"cell_type":"code","source":"class DataLoader(Dataset):\n    def __init__(self, data_mat, labels):\n        self.data_mat = data_mat\n        self.labels = labels\n\n    def __len__(self):\n        return self.data_mat.shape[2]\n\n    def __getitem__(self, idx):\n        mat = np.array(self.data_mat[:,:,idx], dtype=np.float32)\n#         mat_pad = np.zeros((12,13), dtype=np.float32)\n#         mat_pad[1:11, 1:12] = mat\n        mat_pad = np.zeros((12,12), dtype=np.float32)\n        mat_pad[1:11, 1:11] = mat\n        adj_mat = np.expand_dims(mat_pad, 0)\n        label = [self.labels[idx], np.abs(1-self.labels[idx])]\n        return torch.from_numpy(adj_mat), torch.from_numpy(np.array(label, dtype=np.float32))","metadata":{"id":"b5laDFCgqUeV","execution":{"iopub.status.busy":"2024-04-10T13:54:19.470981Z","iopub.execute_input":"2024-04-10T13:54:19.471270Z","iopub.status.idle":"2024-04-10T13:54:19.480851Z","shell.execute_reply.started":"2024-04-10T13:54:19.471245Z","shell.execute_reply":"2024-04-10T13:54:19.479762Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, numChannels=1, classes=2):\n        super().__init__()\n        \n        self.layer1 = nn.Sequential(\n                            Conv2d(in_channels=numChannels, out_channels=32, kernel_size=(2,2)),\n                            ReLU(),\n                            MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n        \n        self.layer2 = nn.Sequential(\n                            Conv2d(in_channels=32, out_channels=64,kernel_size=(2,2)),\n                            ReLU(),\n                            MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n\n        self.flatten1 = Flatten(0,2)\n\n        self.fc1 = nn.Sequential(\n                            Linear(in_features=256, out_features=128),\n                            ReLU())\n\n        self.dropout = Dropout(p=0.2)\n\n        self.fc2 = nn.Sequential(\n                            Linear(in_features=128, out_features=classes),\n                            Sigmoid())\n\n    def forward(self, x):\n      #print(\"x: \", x.shape)\n      x1 = self.layer1(x)\n      #print(\"x1:\", x1.shape)\n      x2 = self.layer2(x1)\n      #print(\"x2:\", x2.shape)\n      f1 = self.flatten1(x2)\n      #print(\"f1:\", f1.shape)\n      f2 = self.fc1(f1)\n      #print(\"f2:\", f2.shape)\n      f3 = self.dropout(f2)\n      #print(\"f3:\", f3.shape)\n      out = self.fc2(f3)\n      #print(\"out:\", out.shape)\n      return out","metadata":{"id":"ynxfKuPsnMb3","execution":{"iopub.status.busy":"2024-04-10T13:54:19.483690Z","iopub.execute_input":"2024-04-10T13:54:19.484399Z","iopub.status.idle":"2024-04-10T13:54:19.495292Z","shell.execute_reply.started":"2024-04-10T13:54:19.484371Z","shell.execute_reply":"2024-04-10T13:54:19.494297Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def train(dataloader, model, loss_fn, optimizer):\n    size = dataloader.__len__\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n        # Compute prediction error\n        pred = model(X)\n        #print(\"pred: \", pred, \"y: \", y)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        loss = loss.item()\n        #print(f\"loss: {loss:>7f}\")","metadata":{"id":"mfSoxnwfqJNd","execution":{"iopub.status.busy":"2024-04-10T13:54:19.496454Z","iopub.execute_input":"2024-04-10T13:54:19.496852Z","iopub.status.idle":"2024-04-10T13:54:19.504790Z","shell.execute_reply.started":"2024-04-10T13:54:19.496818Z","shell.execute_reply":"2024-04-10T13:54:19.503770Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def test(dataloader, model, loss_fn):\n    size = len(dataloader)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    tp = 0\n    fp = 0\n    tn = 0\n    fn = 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            out = pred\n            if(y[0] == 1):\n              if(out[0] >= out[1]):\n                tp += 1\n              elif(out[0] < out[1]):\n                #print(out)\n                fn += 1\n            elif(y[0] == 0):\n              if(out[0] >= out[1]):\n                fp += 1\n              elif(out[0] < out[1]):\n                tn += 1\n    #print(test_loss, num_batches)\n    test_loss /= num_batches\n    correct /= size\n    #print(f\"Test Error: \\n Metrics: {[tp, fp, tn, fn]}, Avg loss: {test_loss:>8f} \\n\")\n    return [tp, fp, tn, fn], test_loss","metadata":{"id":"aGK7Pd0jqL7X","execution":{"iopub.status.busy":"2024-04-10T13:54:19.505982Z","iopub.execute_input":"2024-04-10T13:54:19.506304Z","iopub.status.idle":"2024-04-10T13:54:19.515439Z","shell.execute_reply.started":"2024-04-10T13:54:19.506280Z","shell.execute_reply":"2024-04-10T13:54:19.514587Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# mat = scipy.io.loadmat('dataset/theta_A_W_uni11_100ms_080.mat')[\"W\"]\n\n# # Calculate the number of samples in the data\n# num_samples = mat.shape[2]\n\n# # Labels initialization\n# labels = np.zeros(num_samples)\n\n# # Set first 250 and last 250 labels to 0\n# labels[:250] = 0\n# labels[-250:] = 0\n\n# # Calculate the number of '1's for the middle part\n# num_ones_middle = num_samples - 500\n\n# # Set the middle part labels to 1\n# labels[250:250 + num_ones_middle] = 1\n\n# # Indices setup\n# a1 = np.arange(0, 250)\n# a2 = np.arange(250, 250 + num_ones_middle)\n# a3 = np.arange(250 + num_ones_middle, num_samples)\n\n\n# t1 = list(np.random.choice(a1, int(len(a1)*0.1), replace=False))\n# tr1 = list(set(a1)-set(t1))\n# t2 = list(np.random.choice(a2, int(len(a2)*0.1), replace=False))\n# tr2 = list(set(a2)-set(t2))\n# t3 = list(np.random.choice(a3, int(len(a3)*0.1), replace=False))\n# tr3 = list(set(a3)-set(t3))\n\n# t = t1 + t2 + t3\n# tr = tr1 + tr2 + tr3\n# test_data = mat[:,:,t]\n# train_data = mat[:,:,tr]\n# test_labels = labels[t]\n# train_labels = labels[tr]\n\n# print(train_data.shape)\n\n# print(len(t), len(tr))\n\nimport os\nimport numpy as np\nimport scipy.io\nimport networkx as nx\n\n# Path to the folder containing the files\nfolder_path = '/kaggle/input/dpcn-dataset/dataset'\n# folder_path = '/kaggle/input/dpcn-dataset-euclidean'\n\n# function to calculate various centrality measures of a graph with given adjacency matrix\ndef calculate_centrality_measures(adjacency_matrix):\n    # Convert adjacency matrix to a networkx graph\n    G = nx.from_numpy_array(adjacency_matrix)\n\n#     # Calculate clustering coefficient for each node\n#     centrality_measure = nx.clustering(G)\n\n#     # Calculate eigenvector centrality for each node\n#     centrality_measure = nx.eigenvector_centrality_numpy(G)\n\n#     # Calculate degree centrality for each node\n#     centrality_measure = nx.degree_centrality(G)\n\n#     # Calculate closeness centrality for each node\n#     centrality_measure = nx.closeness_centrality(G)\n\n    # Calculate betweenness centrality for each node\n    centrality_measure = nx.betweenness_centrality(G)\n\n    # Convert eigenvector centralities to an array in the same order as nodes in the graph\n    centralities_array = np.array([centrality_measure[node] for node in range(len(G))])\n\n    return centralities_array\n\n# Lists to append the data and labels\ntrain_data_all = []\ntrain_labels_all = []\ntest_data_all = []\ntest_labels_all = []\n\n# List all files in the folder\nfile_names = os.listdir(folder_path)\n\n# Loop through only one file\nfor file_name in file_names:\n    # Load data from each file\n#     mat1 = scipy.io.loadmat(os.path.join(folder_path, file_name))[\"W\"]\n    mat = scipy.io.loadmat(os.path.join(folder_path, file_name))[\"W\"]\n\n#     mat = np.zeros((10, 11, mat1.shape[2]))\n#     # append the clustering coefficient of the graph to the adjacency matrix\n#     for i in range(mat.shape[2]):\n#         if i == 0:\n#             print(mat1[:,:,i].shape)\n#             print(calculate_centrality_measures(mat1[:,:,i]).flatten().shape)\n#         centrality_measure = calculate_centrality_measures(mat1[:,:,i]).flatten()\n#         # convert (48,48) adjacency matrix to (48,49) matrix\n#         mat[:,:,i] = np.concatenate((mat1[:,:,i], centrality_measure.reshape(10,1)), axis=1)\n#         if i == 0:\n#             print(mat[:,:,i].shape)\n    # Calculate the number of samples in the data\n    num_samples = mat.shape[2]\n    \n    # Labels initialization\n    labels = np.zeros(num_samples)\n    \n    # Set first 250 and last 250 labels to 0\n    labels[:500] = 0\n    labels[-250:] = 0\n    \n    # Calculate the number of '1's for the middle part\n    num_ones_middle = num_samples - 750\n    \n    # Set the middle part labels to 1\n    labels[500:-250] = 1\n    \n#     # Randomly shuffle the labels\n#     np.random.shuffle(labels)\n    \n    # Indices setup\n    a1 = np.arange(0, 500)\n    a2 = np.arange(500, 500 + num_ones_middle)\n    a3 = np.arange(500 + num_ones_middle, num_samples)\n\n    # Splitting indices for test and train sets\n    t1 = list(np.random.choice(a1, int(len(a1) * 0.1), replace=False))\n    tr1 = list(set(a1) - set(t1))\n    t2 = list(np.random.choice(a2, int(len(a2) * 0.1), replace=False))\n    tr2 = list(set(a2) - set(t2))\n    t3 = list(np.random.choice(a3, int(len(a3) * 0.1), replace=False))\n    tr3 = list(set(a3) - set(t3))\n    \n    # Combine indices\n    t = t1 + t2 + t3\n    tr = tr1 + tr2 + tr3\n    \n    # Split data\n    test_data = mat[:, :, t]\n    train_data = mat[:, :, tr]\n    train_labels = labels[tr]\n    test_labels = labels[t]\n\n    print(train_data.shape)\n\n    # Append the data and labels to the lists\n    train_data_all.append(train_data)\n    train_labels_all.append(train_labels)\n    test_data_all.append(test_data)\n    test_labels_all.append(test_labels)\n    break\n\n\nprint(\"=====================================\")\n# Find the maximum number of samples among accumulated data\nmax_samples = max(data.shape[2] for data in train_data_all)\n\n# # Pad zeros at the end of each array to make them all 48x48x(max_samples)\n# train_data_padded = []\n# test_data_padded = []\n# for data in train_data_all:\n#     size = 48 - data.shape[0]\n#     size//=2\n#     padded_data = np.pad(data, ((size, size), (size, size), (0, 0)), mode='constant')\n#     train_data_padded.append(padded_data)\n#     print(padded_data.shape)\n# for data in test_data_all:\n#     size = 48 - data.shape[0]\n#     size//=2\n#     padded_data = np.pad(data, ((size, size), (size, size), (0, 0)), mode='constant')\n#     test_data_padded.append(padded_data)\n\n# Concatenate accumulated data and labels\ntrain_data_concatenated = np.concatenate(train_data_all, axis=2)\ntrain_labels_concatenated = np.concatenate(train_labels_all, axis=0)\ntest_data_concatenated = np.concatenate(test_data_all, axis=2)\ntest_labels_concatenated = np.concatenate(test_labels_all, axis=0)\n\n# Print the shape of concatenated data\nprint(\"Shape of concatenated training data:\", train_data_concatenated.shape)\nprint(\"Shape of concatenated training labels:\", train_labels_concatenated.shape)\nprint(\"Shape of concatenated testing data:\", test_data_concatenated.shape)\nprint(\"Shape of concatenated testing labels:\", test_labels_concatenated.shape)","metadata":{"id":"S3WtJGpD3XE1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bf33e6fe-069b-4756-ac59-77ea47f54fa0","execution":{"iopub.status.busy":"2024-04-10T14:01:07.649053Z","iopub.execute_input":"2024-04-10T14:01:07.649547Z","iopub.status.idle":"2024-04-10T14:01:07.677816Z","shell.execute_reply.started":"2024-04-10T14:01:07.649517Z","shell.execute_reply":"2024-04-10T14:01:07.676820Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"(10, 10, 982)\n=====================================\nShape of concatenated training data: (10, 10, 982)\nShape of concatenated training labels: (982,)\nShape of concatenated testing data: (10, 10, 109)\nShape of concatenated testing labels: (109,)\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Model().to(device)\nprint(model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyaVy6S2_25m","outputId":"429a1561-1b77-49fb-ae2a-b215bfe22056","execution":{"iopub.status.busy":"2024-04-10T14:01:09.839314Z","iopub.execute_input":"2024-04-10T14:01:09.840053Z","iopub.status.idle":"2024-04-10T14:01:09.847717Z","shell.execute_reply.started":"2024-04-10T14:01:09.840020Z","shell.execute_reply":"2024-04-10T14:01:09.846811Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Model(\n  (layer1): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer2): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  )\n  (flatten1): Flatten(start_dim=0, end_dim=2)\n  (fc1): Sequential(\n    (0): Linear(in_features=256, out_features=128, bias=True)\n    (1): ReLU()\n  )\n  (dropout): Dropout(p=0.2, inplace=False)\n  (fc2): Sequential(\n    (0): Linear(in_features=128, out_features=2, bias=True)\n    (1): Sigmoid()\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_data_concatenated, train_labels_concatenated)\ntest_dataloader = DataLoader(test_data_concatenated, test_labels_concatenated)\nmodel = Model().to(device)\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [20], gamma=0.001)\n\nepochs = 50\nmax_c = -1\nmax_c_epoch = -1\nmin_tl = 9999\nmin_tl_epoch = 9999\n\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    #test(train_dataloader, model, loss_fn)\n    c, tl = test(test_dataloader, model, loss_fn)\n    acc = (c[0] + c[2])/np.sum(c)\n    pre = c[0]/(c[0]+c[1]+1)\n    sen = c[0]/(c[0]+c[3]+1)\n    spe = c[2]/(c[1]+c[2])\n    f_score = 2*(pre*sen)/(pre+sen +1)\n    print(\"acc: \", acc, \"pre: \", pre, \"rec: \", sen, \"spe: \", spe, \"f_score: \", f_score, \"c: \", c, \"loss:\", tl)\nprint(\"Done!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22_b33vSqPDe","outputId":"8ebd81e9-1bc9-46c8-e528-728ffc89b712","execution":{"iopub.status.busy":"2024-04-10T14:03:19.316986Z","iopub.execute_input":"2024-04-10T14:03:19.317838Z","iopub.status.idle":"2024-04-10T14:05:10.672739Z","shell.execute_reply.started":"2024-04-10T14:03:19.317808Z","shell.execute_reply":"2024-04-10T14:05:10.671842Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch 1\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6244773093713533\nEpoch 2\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6221659902585756\nEpoch 3\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6226406986013465\nEpoch 4\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6235991289856238\nEpoch 5\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6224864133454244\nEpoch 6\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6224473274629051\nEpoch 7\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6215018094132799\nEpoch 8\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6212457361024454\nEpoch 9\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6208917840358315\nEpoch 10\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6196580329619417\nEpoch 11\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6158331717373035\nEpoch 12\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.615314370721852\nEpoch 13\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6155894130741785\nEpoch 14\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.614562167760429\nEpoch 15\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6137632033146849\nEpoch 16\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6130785912001898\nEpoch 17\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6114507418706876\nEpoch 18\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6105915089266016\nEpoch 19\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6081194032769685\nEpoch 20\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.606673298625771\nEpoch 21\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6047064298336658\nEpoch 22\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6030185630015277\nEpoch 23\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6015358539896274\nEpoch 24\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.599715315694109\nEpoch 25\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.5985618293832201\nEpoch 26\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.5971388494202851\nEpoch 27\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.5960080262717851\nEpoch 28\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.595915917956501\nEpoch 29\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.5944803359858487\nEpoch 30\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.5946001741317434\nEpoch 31\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.5954452049021327\nEpoch 32\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.5967917152501028\nEpoch 33\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.5977964698584801\nEpoch 34\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6009010236471071\nEpoch 35\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.601033314789107\nEpoch 36\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6048359004866093\nEpoch 37\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6080419553802647\nEpoch 38\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6137202914223212\nEpoch 39\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6149715941501867\nEpoch 40\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6242531139506112\nEpoch 41\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6314434015081016\nEpoch 42\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6343479082159219\nEpoch 43\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6365335454156092\nEpoch 44\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6449289812339009\nEpoch 45\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6502241411101107\nEpoch 46\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6556281171602394\nEpoch 47\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6631347534819207\nEpoch 48\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6605519830001467\nEpoch 49\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.6663043057853613\nEpoch 50\n-------------------------------\nacc:  0.6880733944954128 pre:  0.0 rec:  0.0 spe:  1.0 f_score:  0.0 c:  [0, 0, 75, 34] loss: 0.676410342916983\nDone!\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}